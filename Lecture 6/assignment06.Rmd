---
title: "Exercise 6 - Nonlinear Models and Tree-based Methods"
author: Ng Jian Long
output: html_document
---


### Exercise 6.1

This question relates to the `College` dataset from the `ISLR` package.

(a) Split the data into a training set and a test set. Using out-of-state tuition as the response and the other variables as the predictors, perform appropriate model selection of your choice (from day6) on the training set in order to identify a satisfactory model that uses just a subset of the predictors.

```{r, message = FALSE}
library(ISLR)
library(glmnet, quietly = TRUE)
library(caTools, quietly = TRUE)

#Splitting data into training and test set
set.seed(1234)

sample = sample.split(College, SplitRatio = 0.7)
training <- subset(College, sample == TRUE)
test <- subset(College, sample == FALSE)

#Matrix x and vector y for training and test set
x.train <- model.matrix(Outstate ~ ., data = training)[, -1]
y.train <- training$Outstate

x.test <- model.matrix(Outstate ~., data = test)[, -1]
y.test <- test$Outstate

#Lasso Regression
cv.lasso <- cv.glmnet(x.train, y.train, alpha = 1)
lambda.lasso <- cv.lasso$lambda.min
lasso.train <- glmnet(x.train, y.train, alpha = 1, lambda = lambda.lasso)

lasso.coef <- predict(lasso.train, type = "coefficients", s = lambda.lasso)
lasso.coef
```

**Lasso regression is used to perform model selection. The lasso can shrink some of the regression coefficients to zero, effectively selecting a subset of the predictors that are most important for predicting the response variable. This is particularly useful in situations where we have a large number of predictors, and it is not clear which variables are most relevant.**

**Based on the coefficients, only `P.Undergrad` is reduced to zero. All other variables are deemed relevant to the model. This indicates that, while lasso does successfully identify a subset of relevant variables, it might not be the most effective method.**

**A brute force approach was proposed. In particular, the forward subset selection is attempted to identify a satisfactory model with only a subset of variables.**

```{r}
library(leaps)

set.seed(1234)

#Forward selection
reg.fit <-  regsubsets(Outstate ~., data = training, nvmax = 17, method = "forward")
reg.summary <- summary(reg.fit)

#Plotting graphs for Mallow's Cp, AIC and BIC
par(mfrow = c(1, 3))
{
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp", type='l')
min.cp <-  min(reg.summary$cp)
std.cp <-  sd(reg.summary$cp)
abline(h = min.cp + 0.2 * std.cp, col="red", lty=2)
abline(h = min.cp - 0.2 * std.cp, col="red", lty=2)
plot(reg.summary$bic, xlab="Number of Variables", ylab="BIC", type='l')
min.bic <-  min(reg.summary$bic)
std.bic <-  sd(reg.summary$bic)
abline(h = min.bic + 0.2 * std.bic, col="red", lty=2)
abline(h = min.bic - 0.2 * std.bic, col="red", lty=2)
plot(reg.summary$adjr2, xlab = "Number of Variables",
     ylab = "Adjusted R2",type = 'l', ylim = c(0.4, 0.84))
max.adjr2 <-  max(reg.summary$adjr2)
std.adjr2 <-  sd(reg.summary$adjr2)
abline(h = max.adjr2 + 0.2 * std.adjr2, col="red", lty=2)
abline(h = max.adjr2 - 0.2 * std.adjr2, col="red", lty=2)
}
```

**BIC scores show 6 as the optimal size. Cp, BIC and adjr2 show that size 6 is the minimum size for the subset for which the scores are withing 0.2 standard deviations of optimum. Therefore, 6 is chosen as the best subset size. The variable to be included is determined by the coefficients. Only the top 6 variables with highest coefficients are chosen.**

```{r}
reg.fit1 <-  regsubsets(Outstate ~ . , data = College, method = "forward")
reg.coef <-  coef(reg.fit1, id = 6)
names(reg.coef)
```

**The 6 variables to be included are `PrivateYes` `Room.Board`, `PhD`, `perc.alumni`, `Expend` and `Grad.Rate`.**

(b) Fit a GAM on the training data, using out-of-state tuition as the response and the features selected in the previous step as the predictors. Plot the results, and explain your findings.

```{r}
library(gam)

gam.fit <-  gam(Outstate ~ Private + ns(Room.Board, df = 2) + ns(PhD, df = 2) + ns(perc.alumni, df = 2) + 
                  ns(Expend, df = 5) + ns(Grad.Rate, df = 2), data = training)

par(mfrow=c(2, 3))
plot(gam.fit, se = TRUE, col = "blue")
```

**The plots show the estimated smoothing functions for each predictor variable in the model, along with their confidence intervals. The y-axis represents the estimated effect of each predictor on the response variable, while the x-axis represents the range of values of each predictor in the data. The blue line represents the estimated smooth function of the predictor variable. The black lines represent the pointwise standard errors of the estimate, which gives a measure of the uncertainty in the estimate of the smooth function.**

**Given that all the blue lines are located between the two black lines for all graphs, it means that the estimated smooth function is within one standard error of the estimate of the true smooth function. This suggests that the estimate of the smooth function is relatively precise and there is evidence that the true smooth function lies close to the estimated function.**

(c) Evaluate the model obtained on the test set, and explain the results obtained.

```{r}
gam.pred <- predict(gam.fit, test)

gam.err <-  mean((test$Outstate - gam.pred)^2)
gam.tss <-  mean((test$Outstate - mean(test$Outstate))^2)
test.rss <-  1 - gam.err / gam.tss
test.rss
```

**We obtain a test RSS of 0.75 using GAM with 6 predictors.** 

(d) For which variables, if any, is there evidence of a non-linear relationship with the response?

```{r}
summary(gam.fit)
```

**There is evidence of a non-linear relationship with the response for all of the variables in the model that are modeled using natural splines (ns) with degrees of freedom greater than 1. These variables are `Room.Board`, `PhD`, `perc.alumni`, `Expend`, and `Grad.Rate`. The F-tests for the spline terms have very low p-values (< 2.2e-16), indicating strong evidence against the null hypothesis of a linear relationship with the response.**

### Exercise 6.2 

Apply bagging and random forests to a data set of your choice. Be sure to fit the models on a training set and to evaluate their performance on a test set. How accurate are the results compared to simple methods like linear or logistic regression? Which of these approaches yields the best performance?

**The dataset chosen is the `Weekly` stock market price data from the ISLR package.**

```{r}
#Data preparation
set.seed(1234)

train <-  sample(nrow(Weekly), 2/3 * nrow(Weekly))
test <-  -train
```

**Logistic regression**

```{r}
glm.fit <-  glm(Direction ~ . -Year -Today, data = Weekly[train,], family = "binomial")
glm.probs <-  predict(glm.fit, newdata = Weekly[test, ], type = "response")
glm.pred <-  rep("Down", length(glm.probs))
glm.pred[glm.probs > 0.5] <-  "Up"
table(glm.pred, Weekly$Direction[test])
glm.err <- mean(glm.pred != Weekly$Direction[test])
```

**Bagging**

```{r}
library(randomForest, quietly = TRUE)

Weekly <-  Weekly[,!(names(Weekly) %in% c("BinomialDirection"))]

bag.weekly <-  randomForest(Direction~.-Year -Today, data = Weekly, subset = train, mtry = 6)
yhat.bag <-  predict(bag.weekly, newdata = Weekly[test,])
table(yhat.bag, Weekly$Direction[test])
bag.err <- mean(yhat.bag != Weekly$Direction[test])
```

**Random forests**

```{r}
rf.weekly <-  randomForest(Direction ~ . -Year -Today, data = Weekly, subset = train, mtry = 2)

yhat.bag <-  predict(rf.weekly, newdata = Weekly[test,])
table(yhat.bag, Weekly$Direction[test])
rf.err <- mean(yhat.bag != Weekly$Direction[test])
```

```{r}
#Compiling Error Rates
library(knitr)

err.table <- cbind(glm.err, bag.err, rf.err)
colnames(err.table) <- c("Logistic", "Bagging", "Random Forest")
rownames(err.table) <- "Test Error Rate"
kable(err.table)
```

**Logistic Regression has the lowest test error rate.**